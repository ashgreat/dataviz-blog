[
  {
    "path": "posts/2023-10-26-homework-3-2023/",
    "title": "Homework 3 - 2023",
    "description": {},
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": {}
      }
    ],
    "date": "2023-10-27",
    "categories": [],
    "contents": "\nBefore you begin, note that, in the header, the output format of this document is html_notebook. When you save this file, it automatically creates another file with the same file name but with .nb.html extension in the same directory. This is the file you will submit as your homework solution along with the .Rmd file.\n\nWarnings:\nDon‚Äôt delete the nb.html file.\nDon‚Äôt knit your .Rmd file to html. If you want to look at the output, just open the nb.html in the browser. Alternatively, click on the ‚ÄúPreview‚Äù button on top of the document:\n\nIf you delete nb.html file, you may have to create a new .Rmd file and restart from there. If you knit your .Rmd file to html, you will not be able to retain any of the interactivity in the plots. This means the TA will not be able to grade you!\n\nThe objective of this homework is to give you more practice on interactive visualizations using plotly and highcharter.\nAs always, recreate these visualizations exactly. Q1 uses plotly while Q2-Q5 use highcharter.\nQ1 (3 points)\nUse mpg data set from ggplot2 to create a static visualization and then use ggplotly() to create a limited interactive plot.\nHint: You will need to supply only frame. No ids used.\n\n\n\nFor the next four questions, you will use highcharter.\nQ2 (3 points)\nThis example creates a heatmap similar to the one shown here.\nUse mpg data and hchart() function. We want to create a heatmap of average highway mileage for different class and cyl. This plot removes all the observations with five cylinders or with 2seater class. Also note that I am treating cyl as a character (string) variable. This is essential for creating this plot.\nI am using hc_theme_538(). Furthermore, the default color in the heamap is blue, which I changed using hc_colorAxis() function that I used in the Week 10 heatmap.\n\n\n\nQ3 (3 points)\nIn the above plot, the tooltip shows confusing information. Below, I modified the tooltip to present more information. The code is not at all complicated and relies on the tooltip code we used in Week 10.\nNext, I removed the X axis title and modified Y axis title.\nFinally, I added a title to the plot. Note how I used four different emojies related to cars. It doesn‚Äôt matter which car emojis you use as long as they are related to automobiles.\n\n\n\nQ4 (3 points)\nFor this example, use a randomly selected subset of diamonds data set from ggplot2:\n\n\nset.seed(2020)\nd1 = diamonds[sample(nrow(diamonds), 1000),]\n\n\nNext use d1 to create the following plot.\nI have used hc_theme_flat() for this plot. Please use this theme for your plot too!\nYou can add a theme to the plot using hc_add_theme() function. Wherever the word diamond appeared in the plot, I replaced it with the diamond emoji.\nPoint colors in this graph are mapped to clarity. Check out all the variables in this data set by typing ?diamonds in the console.\n\n\n\nQ5 (3 points)\nUsing economics dataset from ggplot2, recreate the following line graph. Learn more about the variables in the dataset by typing ?economics in the console. Here, the Y axis is plotting unemployment.\nI used hc_theme_economist(). You can use any theme you want. You can check out the themes here.\n\n\n\nBonus plot (Not graded)\nThis is the same plot as above except if you hover mouse pointer over the peak of unemployment, the tooltip will show more information. Once again, this is a simple trick and doesn‚Äôt require any advanced coding.\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-10-27T16:24:58-05:00",
    "input_file": "homework-3-2023.knit.md"
  },
  {
    "path": "posts/2023-08-25-workaround-to-link-rstudio-and-github/",
    "title": "Workaround to Linking RStudio and Github",
    "description": "Many students face difficulties in linking RStudio and Github. This post shows a workaround using Github Desktop application.",
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": {}
      }
    ],
    "date": "2023-08-25",
    "categories": [],
    "contents": "\nFor this to work, you must have installed R and RStudio on your computer. Furthermore, you must have a Github.com account. I assume that you already created a repository on Github.com. All the screenshots in this post are taken on a laptop running Windows 10.\nStep 1\nDownload Github Desktop from https://desktop.github.com\n\n\nknitr::include_graphics(here::here(\"Images\", \"github desktop\" ,\"Download Github desktop.png\"))\n\n\n\nStep 2\nLaunch Github Desktop and login using your Github credentials.\n\n\nknitr::include_graphics(here::here(\"Images\", \"github desktop\" ,\"Log in to Github desktop.png\"))\n\n\n\nStep 4\nAuthorize Github Desktop from Github.com opened in your browser.\n\n\nknitr::include_graphics(here::here(\"Images\", \"github desktop\" ,\"Authorize the application.png\"))\n\n\n\nStep 5\nGo back to Github Desktop. You will see the following welcome screen. Select the second option.\n\n\nknitr::include_graphics(here::here(\"Images\", \"github desktop\" ,\"Welcome screen on Github desktop.png\"))\n\n\n\nStep 6\nSelect the repo to clone. Also set the path where this repo will be cloned on your computer. I suggest creating a folder structure such as ‚ÄúMSDA/Fall 2023/Data Visualization‚Äù and then point Github Desktop to this directly. This way, the DA6233-2023 folder will get added to this directory.\n\n\nknitr::include_graphics(here::here(\"Images\", \"github desktop\" ,\"Select the repo to clone.png\"))\n\n\n\nAfter successful repo cloning you should see a similar windows:\n\n\nknitr::include_graphics(here::here(\"Images\", \"github desktop\" ,\"After the repo is cloned.png\"))\n\n\n\nStep 7\nFinally, open RStudio and create a new project from File > New Project. Select the second option labeled ‚ÄúExisting Directory‚Äù. This is because we will use the same directory that we pointed Github Desktop to in Step 6.\n\n\nknitr::include_graphics(here::here(\"Images\", \"github desktop\" ,\"Create a new project.png\"))\n\n\n\nProvide the full link to the project directory and click ‚ÄúCreate Project.‚Äù That‚Äôs it!\n\n\nknitr::include_graphics(here::here(\"Images\", \"github desktop\" ,\"Navigate to the existing directory.png\"))\n\n\n\n\n\n\n",
    "preview": "posts/2023-08-25-workaround-to-link-rstudio-and-github/distill-preview.png",
    "last_modified": "2023-08-25T20:42:29-05:00",
    "input_file": {},
    "preview_width": 1425,
    "preview_height": 972
  },
  {
    "path": "posts/2023-03-05-mkt-3013-score-distribution/",
    "title": "Exam 2 Score Distribution for MKT 3013",
    "description": {},
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": "https://ashwinmalshe.com"
      }
    ],
    "date": "2023-03-05",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-03-04T23:32:19-06:00",
    "input_file": {}
  },
  {
    "path": "posts/installing-r-and-rstudio/",
    "title": "Installing R and RStudio",
    "description": "This is an old post about installing R and RStudio. It still works.",
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": {}
      }
    ],
    "date": "2022-08-08",
    "categories": [],
    "contents": "\nThis a short tutorial for the incoming students of UTSA‚Äôs MS in Data\nAnalytics program. I am going to assume that the reader has no knowledge\nof R and RStudio, the Integrated Development Environment (IDE), which we\nuse to code.\n\nIf you are a Mac user and you are comfortable with command line tools\nusing Terminal, I suggest taking more systematic route for preparing\nyour MacOS for R installation using Homebrew. Instructions are available\nhere.\nOtherwise, just follow the instructions below like everyone else!\n\n\nR is available for download for Linux, MacOS, and Windows from\nthis link: https://cloud.r-project.org. The latest version of R is\n4.2.1 as of 08 August 2022. This is going to be a large\ndownload (about 70 MB) so make sure that you have enough space on the\nhard drive. Also, note that the final installation will take even more\nspace.\nIf you are using older versions of Mac OSX (El Capitan,\nMavericks, etc.), you will find your installation files in the\nsub-directories from this page: https://cloud.r-project.org\nOnce you install R, you can launch it to see whether the\ninstallation is fine. On my Mac, once launched, R looks like\nthis:\n\n\n\nNext we download RStudio IDE. RStudio needs R installation to\nfunction so please make sure that you have successfully followed Steps 1\nthrough 3 above.\nThe latest stable version of RStudio as of 08 August 2022 is\n2022.07.1+554. It can be obtained for all the operating\nsystems from this link: https://www.rstudio.com/products/rstudio/download/#download.\nAgain, the file size is large so make sure that you have enough space on\nthe computer.\nOnce you download RStudio, install the software by opening the\nexe file for Windows and dmg file for Mac. The\ninstallation will be smooth. Just follow the directions, if any, on the\nscreen.\nOnce you launch it you will see a window similar to mine:\n\n\n\nIf RStudio locates the R installation, everything is fine.\nOtherwise, uninstall both R and RStudio and reinstall them in the order\noutlined above. RStudio looks similar to Matlab in case you have used it\nbefore. There is a lot of customization possible for RStudio. You can\ncheck that out from Tools/Global Options menu.\n\n\n\n",
    "preview": "posts/installing-r-and-rstudio/distill-preview.png",
    "last_modified": "2022-08-08T17:41:59-05:00",
    "input_file": {},
    "preview_width": 1312,
    "preview_height": 930
  },
  {
    "path": "posts/2020-11-30-mf-performance/",
    "title": "Persistence in Mutual Fund Performance",
    "description": "In this article, I reproduce Figure 1 from Mark Carhart's classic 1997 Journal of Finance article titled \"On Persistence in Mutual Fund Performance\". The visualizations are created using highcharter and echarts4r packages. It is an interactive bar plot that shows that there is a very low persistence in the mutual fund performance.",
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": "www.ashwinmalshe.com"
      }
    ],
    "date": "2021-10-22",
    "categories": [],
    "contents": "\nMark Carhart‚Äôs classic 1997 Journal of Finance paper is highly cited for the momentum factor used in asset pricing models. Carhart (1997) augmented Fama-French 3-factor model (Fama and French 1993) with a factor that accounts for the momentum anomaly identified by Jegadeesh and Titman (1993).\n\nDownload Carhart‚Äôs article here.\nI read this article for the first time in spring 2007 while taking a class on Capital Markets Research at SUNY-Binghamton taught by Srini Krishnamurthy. Although this article is heavily cited for introducing the momentum factor, the topic of the article is way more interesting. In a nutshell, the article shows that mutual fund managers may not be as skilled as commonly believed, and their performance can be explained away mostly by momentum trading strategies. Furthermore, there is no persistence in mutual fund performance. This means that if you decide to invest in a fund based on the last year‚Äôs returns, the chances are slim that the performance of the fund will persist in future!\nAlthough over the years I may have forgotten some of the details, Figure 1 from this paper is etched in my memory. From a purely data visualization perspective, it is a terrible plot. A 3-dimensional plot on 2-dimensional paper is no go. This is a static plot which means it lacks interactivity. Finally, this plot was printed in black and white! Yet, this is an amazing plot. I reproduce it here and explain why this is so insightful.\n\n\n\nThis plot is based on one-year gross returns of mutual fund. Carhart first ranked these funds in a descending order of the annual returns and then created 10 deciles. Accodingly, Decile 1 contains top 10% funds by returns and Decile 10 has the poorest performing 10% funds. Next he tracks these funds in the succeeding year, where he again sorts them in one-year returns deciles. The plot above shows the transition probability of a fund ranked in decile \\(i\\) in year \\(t\\) moving to decile \\(j\\) in year \\(t+1\\). Formally, he is plotting the following conditional probability: \\(P(j|i)\\). As some of these funds may not survive the next year, \\(j \\in [Decile 1,..., Decile 10, Dead]\\).\nIf mutual funds managers are skilled and their performance is consistent, one would expect that \\(P(j|i) ~ 1\\) when \\(j = i\\). However, the plot above implies something like \\(P(j|i) ~ U(0, 1)\\) meaning a mutual fund may transition freely from one decile in a year to any decile the next year! That‚Äôs quite insightful.\nImproving Figure 1\nMy objective of writing this post is to recreate this data visualization so that the insights are more apparent. In order to do that, I am creating four plots as follows:\nThe first plot is a 2-dimensional heatmap. This is a better representation because it will map the transition probabilities to colors rather than the height of the bars.\nThe second plot uses echarts, a D3 library in JavaScript. I am using an R wrapper called echarts4r. This plot will make this an interactive 3-dimensional visualization.\nI recreate the 3-dimensinoal bar plot using another D3 library called highcharts. I am using an R wrapper called highcharter. With this, I could add different colors to the bars but at the same time, I lost the ability to rotate the plot. I correct it in the last plot.\nIn this plot, I use a Shiny app that adds rotation to the plot made using highcharter.\n\nI used a mutual funds data set spanning 1961 to 2019. In my plots, the deciles are reversed. I created the deciles by sorting returns in ascending order. So Decile 1 consists of poorly performing mutual funds while Decile 10 consists of top performing mutual funds. Additionally, in my data I did not have a quick way to determine dead mutual funds so I don‚Äôt have that category. I just used the reported raw returns without adjusting them for the management expenses.\n\nHeatmap\nThe first plot we will create is a heatmap.\n\n\n\nIn this heatmap, things look a bit more interesting. We can see that the primary and secondary diagonal elements are darker than the off diagonal elements. Following the standard notations in linear algbra, the primary diagonal runs from top left to bottom right. The dark rectangles towards the ends of the diagonal suggest that there is a higher transition probability from low deciles to high deciles and vice versa. The secondary diagonal runs from bottom left to top right. This diagonal captures persistence in performance. The elements on this diagonal are darker suggesting some evidence of persistence. However, the highest probability is 22% for Decile 1. For Decile 9 and 10, these are 13% and 19% respectively. Thus, the evidence for persistence is weak at best. ALso note that although I extended the time period under investigation to 2019, the results are remarkably similar to what Carhart reported in 1997!\n3-Dimensional Bar Plot using echarts4r\nI wanted to more closely replicate Carhart‚Äôs visualization. I decided to use echarts4r to recreate this plot because this package has an easy way to create 3-d plots. However, it has a few limitations as well. R package has only a few functionalities of the original echarts library. I am not proficient in JavaScript so I can‚Äôt make finer changes to the plot. Let‚Äôs give it a shot. Note that the X axis has the deciles in year t, Y axis has the deciles in year t+1, and the Z axis has the probabilities.\n\n\n\nThis plot is not bad at all! It has a lot of details and we can rotate the plot to get an understanding of the tranistion probabilities. Ideally I would have loved to assign the same color to the columns belonging to a decile in year t+1 but I found it difficult to achieve. I will keep exploring this.\n3-Dimensional highcharter Bar Plot\nAs an alternative I decided to use highcharter library which is a wrapper for highcharts. However, there is no easy way to rotate the plot. So I am going to show you two plots - one without rotation and the other with. The one with rotation is actually hosted on Shinyapps.io as a Shiny app.\nNote that in both the plots, the same color is applied to the bars for the decile in year t+1. This is shown in the legend at the bottom of the plots.\nStatic highcharter plot\n\n\n\n\n\n\nWith highcharter, I could assign distinct colors to the deciles. This makes it easier compared to echarts4r to explore transition probabilities. However, we can‚Äôt rotate this plot.\nHow to read this plot? The front row of columns belong to Decile 1 from year t+1. From left to right, we can see the Decile 1 to Decile 10 from year t. Thus, the left most column is the probability that a fund will stay in Decile 1 the next year. The second column from left is the probability that a fund will move from Decile 1 in year t to Decile 2 in year t+1. The second row of columns belongs to Decile 2 from year t+1 and so on.\nDraggable highcharter bar plot\nI decided to host my plot on Shinyapps.io as a Shiny app, which made it possible to drag or rotate the bar plot. Now you are free to play with it! The way to read the plot is the same as the static plot.\nCheck out the webapp here: https://malshe.shinyapps.io/mf-performance/\n\n\n\nConclusion\nFigure 1 from Carhart‚Äôs article is insightful but it suffers from several issues. Using modern technology, we can recreate this figure in multiple ways. I show four ways to recreate it using JavaScript based libraries in R. Although I used data spanning 1961 to 2019, the conclusions from Carhart‚Äôs article remain the same. This shows that his findings were really robust.\n\n\n\nCarhart, Mark M. 1997. ‚ÄúOn Persistence in Mutual Fund Performance.‚Äù The Journal of Finance 52 (1): 57‚Äì82.\n\n\nFama, Eugene F, and Kenneth R French. 1993. ‚ÄúCommon Risk Factors in the Returns on Stocks and Bonds.‚Äù Journal of Financial Economics 33 (1): 3‚Äì56.\n\n\nJegadeesh, Narasimhan, and Sheridan Titman. 1993. ‚ÄúReturns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency.‚Äù The Journal of Finance 48 (1): 65‚Äì91.\n\n\n\n\n",
    "preview": "posts/2020-11-30-mf-performance/distill-preview.png",
    "last_modified": "2021-10-22T23:02:36-05:00",
    "input_file": {},
    "preview_width": 1570,
    "preview_height": 1274
  },
  {
    "path": "posts/2021-10-04-visualization-of-monthly-nber-paper-submissions/",
    "title": "Visualization of monthly NBER paper submissions",
    "description": "This post is a tidytuesday post. See the post below for the data source.",
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": "https://ashwinmalshe.com"
      }
    ],
    "date": "2021-10-04",
    "categories": [],
    "contents": "\nGet the data sets from here: https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-09-28/readme.md\n\nGet the updates about this visualization and full code from here: https://github.com/ashgreat/tidytuesday/tree/main/2021-10-01-NBER\nAnecdotally I had heard that many economists were submitting tons of papers to NBER during the Covid-19 pandemic. However, it was not clear to me whether this was a trend or a flash in the pan. The following visualization show that the spike in 2020 was primarily in one month.\n\n\nShow code\n\npapers %>% \n  count(year, month) %>% \n  mutate(date = as.Date(paste(year, month, \"01\", sep = \"-\"))) %>% \n  ggplot(aes(x = date, y = n)) +\n  geom_line(size = 0.3) +\n  labs(x = \"\", y = \"Number of Submissions\")\n\n\n\n\nWhich month saw the spike? This table shows that it was in May 2020. Top 6 months with highest submissions are in 2020.\n\n\nShow code\n\npapers %>% \n  count(year, month, sort = TRUE) %>% \n  head(10)\n\n\n# A tibble: 10 x 3\n    year month     n\n   <dbl> <dbl> <int>\n 1  2020     5   223\n 2  2020     7   176\n 3  2020    10   168\n 4  2020     6   165\n 5  2020    12   156\n 6  2020     4   152\n 7  2016    12   143\n 8  2013     8   134\n 9  2018     9   134\n10  2015     7   132\n\nLet‚Äôs annotate the earlier plot with the spike in may 2020.\n\n\nShow code\n\npapers %>% \n  count(year, month) %>% \n  mutate(date = as.Date(paste(year, month, \"01\", sep = \"-\"))) %>% \n  ggplot(aes(x = date, y = n)) +\n  geom_line(size = 0.3) +\n  annotate(geom = \"text\", x = as.Date(\"2015-01-01\"), y = 200, \n           label = \"May 2020 had \\n223 submissions\", \n           color = \"#b131a2\", fontface = \"italic\", family = hrbrthemes::font_gs) +\n  labs(x = \"\", y = \"Number of Submissions\")\n\n\n\n\nSub-areas of Economics submitting in May 2020\nNext, let‚Äôs visualize the top subcategories submitting in May 2020. I expected these to be from Heath Care and Heath Economics. But is that the case?\nCreate a merged data set with papers and programs\n\n\nShow code\n\npapers_merged <- papers %>% \n  left_join(paper_programs, by = \"paper\") %>% \n  left_join(programs, by = \"program\")\n\n\n\nNext get the top 10 fields by submissions in May 2020:\n\n\nShow code\n\ntop_10_may_2020 <- papers_merged %>% \n  filter(year == 2020 & month == 5) %>% \n  count(program_desc, sort = TRUE) %>% \n  head(10)\n\n\n\nTake a look at these fields\n\n\nShow code\n\ntop_10_may_2020\n\n\n# A tibble: 10 x 2\n   program_desc                                       n\n   <chr>                                          <int>\n 1 Public Economics                                  62\n 2 Health Economics                                  58\n 3 Economic Fluctuations and Growth                  56\n 4 Labor Studies                                     47\n 5 Corporate Finance                                 34\n 6 Health Care                                       30\n 7 Asset Pricing                                     29\n 8 Development Economics                             25\n 9 International Finance and Macroeconomics          24\n10 Productivity, Innovation, and Entrepreneurship    23\n\nNow we are ready to create a bar plot.\n\n\nShow code\n\ntop_10_may_2020 %>% \n  ggplot(aes(x = reorder(program_desc, n), y = n )) +\n  geom_col() +\n  coord_flip() +\n  labs(x = \"\", y = \"Number of Submissions\")\n\n\n\n\nLet‚Äôs fill the first 3 bars with a different color. Also, using str_swap function from sringr package (bundled with tidyvere so you don‚Äôt have to install it separately) wrap the axis labels.\n\n\nShow code\n\ntop_10_may_2020 %>% \n  mutate(top3 = ifelse(n >= 56, 1, 0)) %>% \n  ggplot(aes(x = reorder(program_desc, n), y = n )) +\n  geom_col(aes(fill = as.factor(top3)), show.legend = FALSE) +\n  coord_flip() +\n  scale_fill_manual(values = c(\"#f78ae0\", \"#b131a2\")) +\n  scale_x_discrete(labels = function(x) str_wrap(x, width = 25)) +\n  labs(x = \"\", y = \"Number of Submissions\")\n\n\n\n\nHow does the distribution of the top 10 most submitted program descriptions look like? First, get the top 10 most popular programs of all times.\n\n\nShow code\n\ntop_10 <-  papers_merged %>% \n  count(program_desc, sort = TRUE) %>%  \n  head(10)\n\n\n\nWe can compare individual distributions using facet_wrap. But there is a better way to compare these distributions over time using ggridges package.\n\n\nShow code\n\npapers_merged %>% \n  inner_join(top_10, by = \"program_desc\") %>% \n  ggplot(aes(x = year, y = program_desc, fill = program)) +\n  geom_density_ridges(scale = 4, alpha = 0.5, size = 0.3) + \n  scale_fill_cyclical(values = c(\"#6638f0\", \"#5cc9f5\")) +\n  scale_y_discrete(labels = function(x) str_wrap(x, width = 25)) +\n  labs(x = \"\", y = \"\")\n\n\n\n\nAlthough almost all the top-10 fields experienced increases in 2020 submissions, International Trade and Finance related submissions were flat. Corporate Finance submissions saw a sharper bump compared to Asset Pricing, which both belong to Finance.\n\n\n\n",
    "preview": "posts/2021-10-04-visualization-of-monthly-nber-paper-submissions/visualization-of-monthly-nber-paper-submissions_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-10-04T11:49:26-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-09-27-kiyosaki/",
    "title": "Kiyosaki Predicting Stock Market Crash",
    "description": "This post is WIP",
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": "https://ashwinmalshe.com"
      }
    ],
    "date": "2021-10-02",
    "categories": [],
    "contents": "\n\n\n\nRobert Kiyosaki is a the bestselling author of many personal finance books, most notably Rich Dad Poor Dad. But being good at personal finance doesn‚Äôt mean that he is good at predicting stock market! Over the last 1.5 years, Kiyosaki has warned at least 19 times about an impending stock market crash. As it happens, many of his predictions were made at the bottom of the market. As such, if you followed his advice to move to cash, you would have lost out on the market run up.\nKiyosaki has a large fan following and enjoys strong loyalty from them. Many in the news media take his warnings seriously without looking at his past history such as this article from Entrepreneur. About a week ago, the parody #FinTwit account Dr.¬†Parik Patel shared the following image on Twitter that annotated S&P 500 Index with Kiyosaki‚Äôs tweets about market crash.\n\n\nImagine taking investment advice from Robert Kiyosaki pic.twitter.com/eXqacIDnPG\n\n‚Äî Dr.¬†Parik Patel, BA, CFA, ACCA Esq. üí∏ (@ParikPatelCFA) September 26, 2021\n\nAlthough this visualization is pretty cool, it‚Äôs difficult to read because it‚Äôs static and has many objects on the chart. So I took this opportunity to create a interactive visualization using highcharter, highcharts, and R. In the line graph below, you can hover your mouse on the blue points to see the exact warning Kiyosaki tweeted. Enjoy playing with it!\n\n\n\nCode for making the plot\nTo make the above plot, you will need two things:\nS&P 500 Index returns\nScreenshots of Kiyosaki‚Äôs tweets\nI downloaded the S&P 500 returns from Yahoo Finance. For Kiyosaki‚Äôs tweets, I searched for the terms ‚Äúcrash‚Äù and ‚Äúcrashed‚Äù on his Twitter timeline and went over the tweets that pertain to stock market crash. He has been using that word a lot for prices in many other assets like Bitcoin and gold. I did not use those tweets.\nYou can download the data set as an RDS file from my Github repository for this blog as shown below. Next you are ready to make the chart!\n\n\n# Read the data set\ndt <- readRDS(url(\"https://github.com/ashgreat/dataviz-blog/blob/main/data/kiyosaki_crash.rds?raw=true\"))\n\n# Make the chart!\n\ndt %>% \n  hchart(\"line\", hcaes(Date, Adj_Close),\n         enableMouseTracking = FALSE,\n         states = list(inactive = list(opacity = 1))) %>% \n  hc_add_series(dt, \"point\", hcaes(Date, Adj_Close2)) %>% \n  hc_yAxis(title = list(text = \"S&P 500 Adjusted Close\")) %>% \n  hc_caption(text = '<em>Made by <b>Ashwin Malshe <\/b> <a href=\"www.dataviz.school\">www.dataviz.school<\/a><\/em>') %>% \n  hc_title( text = \"Kiyosaki Predicting Market Crash\") %>% \n  hc_tooltip( useHTML = TRUE,\n              formatter = JS(\"function(){return(this.point.tweet_tooltip2)}\"),\n              shape = \"square\",\n              borderWidth = 0,\n              backgroundColor = NULL,\n              borderColor = NULL) %>%\n  hc_add_theme(hc_theme_538())\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-02T17:16:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-28-ggplot2-ptactice-problem-set/",
    "title": "ggplot2 Ptactice Problem Set",
    "description": "This used to be a homework assignment in DA 6233. I converted it into a problem set.",
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": {}
      }
    ],
    "date": "2021-09-28",
    "categories": [],
    "contents": "\nIn this homework, you will use diamonds data set from ggplot2 package. It is automatically loaded when you execute library(ggplot2) so you don‚Äôt have to separately load it. Please create a duplicate of diamonds and use that for homework. This will avoid corrupting the original data set.\nMake sure that you understand the variables in the data by checking the help file by executing help(diamonds) in the console. The data has five variables that measure the dimensions. The diagram below explains these dimensions.\n\n\nShow code\n\nknitr::include_graphics(\"Diamonds.png\")\n\n\n\n\nInstructions\nYou are expected to recreate each plot exactly as shown in this homework.\nThe objective of this homework is to help you develop fine grain control over ggplot2. As such, please refrain from exercising artistic freedom!\nAll the plots use theme_minimal(). You can set this as your default theme by adding this line in the setup chunk after you load ggplot2 library: theme_set(theme_minimal())\nQ1\nRecreate the following graph. The parameter that controls transparency is set at 0.3. You need not get exactly the same colors but they must be discrete and should not follow a color gradient.\nHint: The points are hollow circles. Recall that we covered this specific shape in the last class.\nHint: Note that the Y axis text is modified.\n\n\nShow code\n\nggplot(d1, aes(carat, price)) +\n  geom_point(aes(fill = as.character(clarity)), shape = 21 ,alpha = 0.3) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(x = \"Diamond Carats\", \n       y = \"Diamond Price\",\n       title = \"Scatterplot of Diamond Prices\")\n\n\n\n\nQ2\nThe previous graph looks cluttered. So you decided to use facets instead. Recreate the following graph:\n\n\nShow code\n\nggplot(d1, aes(carat, price)) +\n  geom_point(aes(fill = as.character(clarity)),\n             shape = 21,\n             alpha = 0.3) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  facet_wrap(~clarity, nrow = 2) +\n  labs(x = \"Diamond Carats\", \n       y = \"Diamond Price\",\n       title = \"Scatterplot of Diamond Prices\") +\n  theme(legend.position = \"none\")\n\n\n\n\nQ3\nNext, you want to know whether the price of diamonds depends on table and depth. Note the line types. Recreate the following graphs:\nHint: Pay attention to the X axis limits. Also, read up more on linetype here. Both Q3 plots use geom_smooth.\n\n\nShow code\n\nggplot(d1, aes(table, price)) +\n  geom_smooth(method = \"lm\", \n              color = \"red\", \n              linetype = \"dashed\") +\n  scale_x_continuous(limits = c(0, 100)) +\n  xlab(\"Table\") + ylab(\"Price\") \n\n\n\n\nHint: Pay attention to the X axis limits and breaks.\n\n\nShow code\n\nggplot(d1, aes(depth, price)) +\n  geom_smooth(method = \"lm\", \n              color = \"white\", \n              linetype = \"dotdash\") +\n  scale_x_continuous(limits = c(0, 80), breaks = seq(0, 80, 10)) +\n  xlab(\"Depth\") + ylab(\"Price\") \n\n\n\n\nQ4\nRecreate each of the following graphs for data exploration:\nHint: In this plot, change the colors manually. Use any colors you want but I have used a color palette with these colors: 99B898, FECEAB, FF847C, E84A5F, 2A363B. If you decide to use them, don‚Äôt forget to add a # prefix to each color code.\n\n\nShow code\n\nggplot(d1, aes(x = x*y*z, price)) +\n  geom_point(aes(color = cut)) +\n  scale_color_manual(values = c(\"#99B898\", \"#FECEAB\", \"#FF847C\", \"#E84A5F\", \"#2A363B\")) \n\n\n\n\nHint: Pay attention to the axes text formatting\n\n\nShow code\n\nggplot(d1, aes(price)) + \n  geom_histogram(bins = 75, color = \"white\") +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  scale_y_continuous(labels = scales::comma_format())\n\n\n\n\nHint: Note that the bars have a distinct border. You can use any color you want. Note also that the legend is not visible.\n\n\nShow code\n\nggplot(d1, aes(clarity)) + \n  geom_bar(aes(fill = clarity), color = \"#c61c5a\") + \n  theme(legend.position = \"none\")\n\n\n\n\nHint: Here you have to use two new geoms for the violin plot and the jittered points.\n\n\nShow code\n\nggplot(d1, aes(cut, depth)) + \n  geom_violin(color = \"blue\") + \n  geom_jitter(alpha = 0.05, \n              color = \"red\")\n\n\n\n\nHint: This plot uses two layers of geom_smooth.\n\n\nShow code\n\nggplot(d1, aes(x, price)) +\n  geom_smooth(se = FALSE) +\n  geom_smooth(method = \"lm\", \n              color = \"green\", \n              se = FALSE)\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-09-28-ggplot2-ptactice-problem-set/Diamonds.png",
    "last_modified": "2021-09-30T16:03:37-05:00",
    "input_file": {},
    "preview_width": 2108,
    "preview_height": 1215
  },
  {
    "path": "posts/2021-09-12-homework0-solution/",
    "title": "Solution to Homework0 ",
    "description": "This is the solution of MSDA Homework0 from 09/2020",
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": "www.ashwinmalshe.com"
      }
    ],
    "date": "2021-09-12",
    "categories": [],
    "contents": "\nThis homework uses txhousing dataset from ggplot2 package, which is a part of the tidyverse. Additionally, it uses Consumer Price Index (CPI) dataset, which I downloaded from the BLS website (https://data.bls.gov/timeseries/CUUR0000SA0) and cleaned. I will provide the code for cleaning CPI data in the solution to this homework.\ntxhousing consists of the following variables:\n\n\nnames(txhousing)\n\n\n[1] \"city\"      \"year\"      \"month\"     \"sales\"     \"volume\"   \n[6] \"median\"    \"listings\"  \"inventory\" \"date\"     \n\nBefore you start working on this homework, study the variables in txhousing as well as the structure of the dataset by typing this command in your console.\n\n\nhelp(txhousing)\n\n\n\nTake a peek at the data by typing:\n\n\nhead(txhousing)\n\n\n\n\n\ncpi <- readxl::read_excel(\"cpi.xlsx\", skip = 11) %>% \n  select(-c(14:15)) %>% \n  janitor::clean_names() %>% \n  pivot_longer(cols = !year, names_to = \"month_name\", values_to = \"cpi\")\n\nsaveRDS(cpi, file = \"cpi.rds\")\n\n\n\n\n\ncpi <- readRDS(\"cpi.rds\")\n\n\n\n\n\n\nSave the CPI dataset in your project folder where you have also saved this homework file. Read the CPI dataset into this session. It consists of the following columns:\n\n\nnames(cpi)\n\n\n[1] \"year\"       \"month_name\" \"cpi\"       \n\nTake a look at the first few observations by using head() function.\n\nThis homework consists of 10 questions and each carries one point. Your objective is to reproduce the output shown in the HTML file for Q1 through Q9. For Q10 just print the name of the city and the value.\n\nDollar values over a long time series make comparison difficult due to inflation. $100 in January 2000 is worth $154 in July 2020. In txhousing, there are two variables‚Äîvolume and median‚Äîwhich are specified in unadjusted USD. Questions 1 to 4 are designed to inflation-adjust these two variables to July 2020 dollars.\nQ1\nCreate a new data frame month_map with 12 rows and 2 columns titled month_name and month. month_name in month_map should have only the unique values from month_name column in CPI data frame. month column should contain the month numbers from 1 to 12.\nTry to not type out this data frame manually and instead try to do it algorithmically. This is how it will look:\n\n\nmonth_map <- data.frame(month_name = cpi$month_name[1:12],\n                        month = 1:12)\n\nhead(month_map, 4)\n\n\n  month_name month\n1        jan     1\n2        feb     2\n3        mar     3\n4        apr     4\n\nQ2\nMerge month_map to the CPI data frame. Explicitly identify the common key variable on which you will perform the merge. Store the resulting merged data frame as cpi_merge. Print the first six rows.\nHere are the first six rows of cpi_merge:\n\n\ncpi_merge <- cpi %>% \n  inner_join(month_map, by = \"month_name\")\n\nhead(cpi_merge)\n\n\n# A tibble: 6 x 4\n   year month_name   cpi month\n  <dbl> <chr>      <dbl> <int>\n1  1999 jan         164.     1\n2  1999 feb         164.     2\n3  1999 mar         165      3\n4  1999 apr         166.     4\n5  1999 may         166.     5\n6  1999 jun         166.     6\n\nQ3\nWe want to add a column to the txhousing data frame that holds the information on the CPI. Rather than altering txhousing, create a new data frame housing by merging txhousing and cpi_merge. The common keys for merging are year and month. Note that the resulting data frame is essentially txhousing with just one more column of CPI.\nHere are the top six rows of housing:\n\n\nhousing <- txhousing %>% \n  inner_join(cpi_merge, by = c(\"year\", \"month\")) %>% \n  select(!month_name)\n\nhead(housing)\n\n\n# A tibble: 6 x 10\n  city   year month sales volume median listings inventory  date   cpi\n  <chr> <dbl> <int> <dbl>  <dbl>  <dbl>    <dbl>     <dbl> <dbl> <dbl>\n1 Abil‚Ä¶  2000     1    72 5.38e6  71400      701       6.3 2000   169.\n2 Abil‚Ä¶  2000     2    98 6.50e6  58700      746       6.6 2000.  170.\n3 Abil‚Ä¶  2000     3   130 9.28e6  58100      784       6.8 2000.  171.\n4 Abil‚Ä¶  2000     4    98 9.73e6  68600      785       6.9 2000.  171.\n5 Abil‚Ä¶  2000     5   141 1.06e7  67300      794       6.8 2000.  172.\n6 Abil‚Ä¶  2000     6   156 1.39e7  66900      780       6.6 2000.  172.\n\nQ4\nModify housing by adding these new columns:\ncpi_latest - Contains the CPI of July 2020. This entire column will have the same value appearing in each cell.\nmultiplier - Ratio of cpi_latest and cpi\nvolume_adj - Adjusted volume as a product of volume and multiplier\nmedian_adj - Adjusted median sale price as a product of median and multiplier\nHere I show the top six rows with only a select few columns to help you ensure your output resembles this:\n\n\njuly_2020_cpi <- cpi_merge %>% \n  filter(year == 2020, month == 7) %>% \n  pull(cpi)\n\nhousing <- housing %>% \n  mutate(cpi_latest = july_2020_cpi,\n         multiplier = cpi_latest / cpi,\n         volume_adj = volume * multiplier,\n         median_adj = median * multiplier)\n\nhousing %>% \n  select(city, year, month, volume, median, \n         cpi_latest, multiplier, volume_adj, median_adj) %>% \n  head()\n\n\n# A tibble: 6 x 9\n  city     year month   volume median cpi_latest multiplier volume_adj\n  <chr>   <dbl> <int>    <dbl>  <dbl>      <dbl>      <dbl>      <dbl>\n1 Abilene  2000     1  5380000  71400       259.       1.53   8258077.\n2 Abilene  2000     2  6505000  58700       259.       1.53   9926101.\n3 Abilene  2000     3  9285000  58100       259.       1.51  14052294.\n4 Abilene  2000     4  9730000  68600       259.       1.51  14717179.\n5 Abilene  2000     5 10590000  67300       259.       1.51  15999298.\n6 Abilene  2000     6 13910000  66900       259.       1.50  20905423.\n# ‚Ä¶ with 1 more variable: median_adj <dbl>\n\nQ5\nUsing housing from Q4, create a new data frame housing_sum1 with this information for each city across all years and months:\nMaximum and minimum volume_adj\nMaximum and minimum median_adj sale price\nHint: If you group by city, you will get the summary across all the years and months. \nMerge housing_sum1 into housing by city and save it as a new dataset housing_1.\nHere I show first six rows of housing_1 and only a select columns:\n\n\nhousing_sum1 <- housing %>% \n  group_by(city) %>% \n    summarize(volume_adj_max = max(volume_adj),\n              volume_adj_min = min(volume_adj),\n              median_adj_max = max(median_adj),\n              median_adj_min = min(median_adj),\n              .groups = \"drop\")\n\nhousing_1 <- housing %>% \n  inner_join(housing_sum1, by = \"city\")\n\n\nhousing_1 %>% \n  select(city, year, month, volume_adj_max, \n         volume_adj_min, median_adj_max, median_adj_min) %>% \n  head()\n\n\n# A tibble: 6 x 7\n  city     year month volume_adj_max volume_adj_min median_adj_max\n  <chr>   <dbl> <int>          <dbl>          <dbl>          <dbl>\n1 Abilene  2000     1      49773624.       7678915.        161440.\n2 Abilene  2000     2      49773624.       7678915.        161440.\n3 Abilene  2000     3      49773624.       7678915.        161440.\n4 Abilene  2000     4      49773624.       7678915.        161440.\n5 Abilene  2000     5      49773624.       7678915.        161440.\n6 Abilene  2000     6      49773624.       7678915.        161440.\n# ‚Ä¶ with 1 more variable: median_adj_min <dbl>\n\nHere I show last six rows of housing_1 and only a select columns:\n\n\nhousing_1 %>% \n  select(city, year, month, volume_adj_max, \n         volume_adj_min, median_adj_max, median_adj_min) %>% \n  tail()\n\n\n# A tibble: 6 x 7\n  city         year month volume_adj_max volume_adj_min median_adj_max\n  <chr>       <dbl> <int>          <dbl>          <dbl>          <dbl>\n1 Wichita Fa‚Ä¶  2015     2      33832205.       8337061.        147084.\n2 Wichita Fa‚Ä¶  2015     3      33832205.       8337061.        147084.\n3 Wichita Fa‚Ä¶  2015     4      33832205.       8337061.        147084.\n4 Wichita Fa‚Ä¶  2015     5      33832205.       8337061.        147084.\n5 Wichita Fa‚Ä¶  2015     6      33832205.       8337061.        147084.\n6 Wichita Fa‚Ä¶  2015     7      33832205.       8337061.        147084.\n# ‚Ä¶ with 1 more variable: median_adj_min <dbl>\n\nQ6\nUsing housing_1 from Q5, create a new data frame housing_min which will retain only the rows of housing_1 where volume_adj of a city was equal to the minimum adjusted volume.\nHere I show first six rows of housing_min and only a select columns:\n\n\nhousing_min <- housing_1 %>% \n  filter(volume_adj == volume_adj_min)\n\nhousing_min %>% \n  select(city, year, month, volume_adj, \n         volume_adj_min, volume_adj_max) %>% \n  head()\n\n\n# A tibble: 6 x 6\n  city       year month volume_adj volume_adj_min volume_adj_max\n  <chr>     <dbl> <int>      <dbl>          <dbl>          <dbl>\n1 Abilene    2003     1   7678915.       7678915.      49773624.\n2 Amarillo   2005    10   9397614.       9397614.      67596289.\n3 Arlington  2011     1  30008080.      30008080.     137867140.\n4 Austin     2009     1 252191554.     252191554.    1248942028.\n5 Bay Area   2000     1  45009066.      45009066.     214278169.\n6 Beaumont   2001     1  13117820.      13117820.      57413784.\n\nUsing housing_1 from Q5, create a new data frame housing_max which will retain only the rows of housing_1 where median_adj of a city was equal to the maximum adjusted median sale price.\nHere I show first six rows of housing_max and only a select columns:\n\n\nhousing_max <- housing_1 %>% \n  filter(median_adj == median_adj_max)\n\nhousing_max %>% \n  select(city, year, month, median_adj, \n         median_adj_max, median_adj_min) %>% \n  head()\n\n\n# A tibble: 6 x 6\n  city       year month median_adj median_adj_max median_adj_min\n  <chr>     <dbl> <int>      <dbl>          <dbl>          <dbl>\n1 Abilene    2015     7    161440.        161440.         84890.\n2 Amarillo   2015     5    172040.        172040.        113162.\n3 Arlington  2015     6    195435.        195435.        131800.\n4 Austin     2015     4    296007.        296007.        200091.\n5 Bay Area   2015     7    218004.        218004.        154570.\n6 Beaumont   2010     1    195862.        195862.        108773.\n\nQ7\nUsing housing_1 from Q5, create a new data frame housing_sum2 with this information for each year and month pair across all cities:\nMedian listings\nMedian sales\nHint: If even a single value for listings or sales of a city is NA, the median of that variable will be NA. In order to avoid this, use na.rm = TRUE argument in median(). \nHere I show first six rows of housing_sum2 and all the columns:\n\n\nhousing_sum2 <- housing_1 %>% \n  group_by(year, month) %>% \n  summarize(listings_med = median(listings, na.rm = TRUE),\n            sales_med = median(sales, na.rm = TRUE),\n            .groups = \"drop\") \n\nhead(housing_sum2)\n\n\n# A tibble: 6 x 4\n   year month listings_med sales_med\n  <dbl> <int>        <dbl>     <dbl>\n1  2000     1         972         99\n2  2000     2         916.       134\n3  2000     3         946.       167\n4  2000     4         985        153\n5  2000     5         978.       165\n6  2000     6         864.       188\n\nMerge housing_sum2 into housing_1 and save a new data frame housing_2.\nHere I show first six rows of housing_2 and some of the columns:\n\n\nhousing_2 <- housing_1 %>% \n  inner_join(housing_sum2, by = c(\"year\", \"month\"))\n\nhousing_2 %>% \n  select(city, year, month, listings, \n         sales, listings_med, sales_med) %>% \n  head()\n\n\n# A tibble: 6 x 7\n  city     year month listings sales listings_med sales_med\n  <chr>   <dbl> <int>    <dbl> <dbl>        <dbl>     <dbl>\n1 Abilene  2000     1      701    72         972         99\n2 Abilene  2000     2      746    98         916.       134\n3 Abilene  2000     3      784   130         946.       167\n4 Abilene  2000     4      785    98         985        153\n5 Abilene  2000     5      794   141         978.       165\n6 Abilene  2000     6      780   156         864.       188\n\nQ8\nModify housing_2 from Q7 to add these indicator variables (also called dummy variables):\nlistings_ind - If a city‚Äôs listings is less than or equal to the median listings for that year and month across all the cities, the value should be 0 else it should be 1.\nsales_ind - If a city‚Äôs sales is less than or equal to the median sales for that year and month across all the cities, the value should be 0 else it should be 1.\nHint: This can be achieved using ifelse() function from R along with mutate() from dplyr\nHere I show first six rows of housing_2 and some of the columns:\n\n\nhousing_2 <- housing_2 %>% \n  mutate(listings_ind = ifelse(listings <= listings_med, 0, 1),\n         sales_ind = ifelse(sales <= sales_med, 0, 1))\n\nhousing_2 %>% \n  select(city, year, month, listings, \n         listings_med, listings_ind,\n         sales, sales_med, sales_ind) %>% \n  head()\n\n\n# A tibble: 6 x 9\n  city   year month listings listings_med listings_ind sales sales_med\n  <chr> <dbl> <int>    <dbl>        <dbl>        <dbl> <dbl>     <dbl>\n1 Abil‚Ä¶  2000     1      701         972             0    72        99\n2 Abil‚Ä¶  2000     2      746         916.            0    98       134\n3 Abil‚Ä¶  2000     3      784         946.            0   130       167\n4 Abil‚Ä¶  2000     4      785         985             0    98       153\n5 Abil‚Ä¶  2000     5      794         978.            0   141       165\n6 Abil‚Ä¶  2000     6      780         864.            0   156       188\n# ‚Ä¶ with 1 more variable: sales_ind <dbl>\n\nQ9\nUsing housing_2 from Q8, add a new variable market_hotness as follows:\n\n\nhot_map <- data.frame(listings_ind = c(0, 0, 1, 1),\n           sales_ind = c(0, 1, 0, 1), \n           market_hotness = c(\"Low\", \"High\", \"Very Low\", \"Average\")) \nhot_map %>% \n  knitr::kable(align = c(\"c\", \"c\", \"c\"))\n\n\nlistings_ind\nsales_ind\nmarket_hotness\n0\n0\nLow\n0\n1\nHigh\n1\n0\nVery Low\n1\n1\nAverage\n\nHere I show first six rows of housing_2 and some of the columns:\n\n\nhousing_2 <- housing_2 %>% \n  inner_join(hot_map, by = c(\"listings_ind\", \"sales_ind\"))\n\nhousing_2 %>% \n  select(city, year, month, listings, sales,\n         listings_ind, sales_ind, market_hotness) %>% \n  head()\n\n\n# A tibble: 6 x 8\n  city     year month listings sales listings_ind sales_ind\n  <chr>   <dbl> <int>    <dbl> <dbl>        <dbl>     <dbl>\n1 Abilene  2000     1      701    72            0         0\n2 Abilene  2000     2      746    98            0         0\n3 Abilene  2000     3      784   130            0         0\n4 Abilene  2000     4      785    98            0         0\n5 Abilene  2000     5      794   141            0         0\n6 Abilene  2000     6      780   156            0         0\n# ‚Ä¶ with 1 more variable: market_hotness <chr>\n\nQ10\nWhich city has the highest average median_adj sale price and what is that price?\n\n\nhousing_2 %>% \n  group_by(city) %>% \n  summarize(median_adj_avg = mean(median_adj, na.rm = TRUE),\n            .groups = \"drop\") %>% \n  arrange(-median_adj_avg) %>% \n  head(1)\n\n\n# A tibble: 1 x 2\n  city          median_adj_avg\n  <chr>                  <dbl>\n1 Collin County        252324.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-12T23:16:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-31-msda-faculty/",
    "title": "MSDA Faculty",
    "description": "A list of MSDA full-time faculty",
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": "www.ashwinmalshe.com"
      }
    ],
    "date": "2021-08-18",
    "categories": [],
    "contents": "\nWe don‚Äôt have an official listing for the faculty teaching in the MS in Data Analytics (MSDA) program at UTSA as we don‚Äôt have any professors permanently assigned to the program. However, there are a few professors, including me, who have been teaching regularly in MSDA. The following table lists these professors and the links to their profiles.\n\n\nCourse\n\n\nFaculty\n\n\nDA6213. Data-Driven Decision Making and Design\n\n\nYeonjoo Park\n\n\nDA6223. Data Analytics Tools and Techniques\n\n\nWenbo Wu\n\n\nIsil Koyuncu\n\n\nDA6233. Data Analytics Visualization and Communication\n\n\nAshwin Malshe\n\n\nDA6813. Data Analytics Applications\n\n\nArka Roy\n\n\nAshwin Malshe\n\n\nDA6823. Data Analytics Practicum I\n\n\nMax Kilger\n\n\nDA6833. Data Analytics Practicum II\n\n\nMax Kilger\n\n\nAshwin Malshe\n\n\nIS6713. Data Foundations\n\n\nAnthony Rios\n\n\nIS6733. Deep Learning on Cloud Platforms\n\n\nYuanxiong (Richard) Guo\n\n\nSTA6443. Statistical Modeling\n\n\nYeonjoo Park\n\n\nSTA 6543. Predictive Modeling\n\n\nMin Wang\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-18T14:10:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-12-ggorce/",
    "title": "Easy pie charts and donut charts with ggforce",
    "description": "I make pie charts and donut charts in ggplot2 using a combination of a bar plot and polar coordinates. This works fine but polar coordinates prevent using any other geoms or annotations that need Cartesian coordinates. For example, annoatation_raster doesn't work with polar coordinates. ggforce solves this issue.",
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": "www.ashwinmalshe.com"
      }
    ],
    "date": "2020-12-12",
    "categories": [],
    "contents": "\nIn the previous post, I created a geofacet of donut charts using coord_polar() function from ggplot2. You can also create pie charts in the same way. However, there are two issues with this method.\nThe method is not intuitive. Whether you will get a pie chart or a donut chart depends on xlim() which has no apparent connection to how the resulting plot will look like. This is confusing irrespective of how long you have been using ggplot2.\nggplot2 can handle only one coordinate system per plot. This means, you can‚Äôt have polar coordinates and Cartesian coordinates in the same plot. This makes sense because these are two totally different coordinate systems. However, this creates problems when you want to use layers that belong to separate coordinate systems. For instance, it is impossible to overlay images on top of the donut charts. This is because annoate_raster() in ggplot2, which is used to insert images, doesn‚Äôt work with polar coordinates. It needs Cartesian coordinates. On the other hand, the donut charts were created using polar coordinates. Otherwise, it is just a bar plot in Cartesian coordinates!\nIn this post I show you how to overcome both these issues by using a relatively unknown package called ggforce. The developer of this package, Thomas Lin Pedersen, is in the core development team for ggplot2. He is also the author of gganimate and patchwork packages.\nWe will use geom_arc_bar() function from ggforce to create pie charts and donut charts. Since it uses Cartesian coordinate system, including images in the plot is super simple.\nData preparation\nLet‚Äôs create a simple data set with electoral votes of Biden and Trump. I am using this example because you are familiar with the general idea from my previous post.\n\n\ndt <- tibble(candidate = c(\"Joe Biden\", \"Donald Trump\"),\n             electoral_votes = c(306, 232))\n\n\n\nPie chart\nLet‚Äôs make a pie chart first. With two categories, a pie chart is not a bad choice for visualization. There is not a lot to explain in the code below. Both pie and donut charts are circles. To plot a circle, we need only three parameters. We need the x and y coordinates of the center of the circle and we need the radius. A donut chart requires one more parameter. Imagine a donut as an image with two concentric cirles. So for a donut chart, we need to provide the radius of the inner circle as well.\ngeom_arc_bar() can be used for making pie charts and donut charts. It requires an aes() function with the following arguments for positioning circles:\nx0: X coordinate of the center of the circle\ny0: Y coordinate of the center of the circle\nr0: Radius of the inner circle\nr: Radius of the outer circle\nThe other arguments, amount and fill are self explanatory. The variable mapped to amount will decide the size of the pie. In Tableau terminology, this will be mapped to the angle of the pie or donut.\nstat = \"pie\" specifies the that we want a pie chart. When r0 = 0, we get a pie chart. When r0 > 0 we get the inner circle, which results in a donut chart.\nApart from geom_arc_bar() you also need coord_fixed(). This is required to make sure that there is no scaling along the X and Y axis and you indeed get a circle. Otherwise you will get an oval rather than a circle.\nHere I set x0 and y0 both to 0. Of course, this is not a requirement and it will change based on the complexity of the visualization you are making. It‚Äôs just the position of the circle on X and Y axes. Next, I set r = 1. Once again, it depends on the context and complexity of your visualization.\n\n\nggplot(dt) +\n  geom_arc_bar(aes(x0 = 0, y0 = 0, r0 = 0, r = 1, \n                   amount = electoral_votes, \n                   fill = candidate), \n               stat = 'pie',\n               color = \"white\") +\n  geom_label(x = c(0.5, -0.5), y = c(-0.3, 0.3), aes(label = electoral_votes)) +\n  scale_fill_manual(values = c(\"#004BA8\", \"#e63946\")) +\n  theme_void() + \n  coord_fixed() +\n  theme(legend.position = \"none\")\n\n\n\n\nThis is a nice pie chart. I positioned the labels such that they are on a roughly 135¬∫ line.\nDonut chart\nNow let‚Äôs make the donut chart. The code below looks almost the same as the previous code except we have set r0 = 0.8. You will also have to adjust the label positions, which I did by doing some trial and error.\n\n\nggplot(dt) +\n  geom_arc_bar(aes(x0 = 0, y0 = 0, r0 = 0.8, r = 1, \n                   amount = electoral_votes, \n                   fill = candidate), \n               stat = 'pie',\n               color = \"white\") +\n  geom_label(x = c(0.85, -0.85), y = c(-0.3, 0.3), aes(label = electoral_votes)) +\n  scale_fill_manual(values = c(\"#004BA8\", \"#e63946\")) +\n  theme_void() + \n  coord_fixed() +\n  theme(legend.position = \"none\")\n\n\n\n\nDonut chart with image\nThe two graphs above solved the first issue I identified ‚Äî these two graphs are much easier and intuitive to make compared to using coord_polar(). Next, let‚Äôs include an image in the donut chart. I recommend finding an image using Google Images, Pixabay, or Unsplash. I also found that making the image square makes adding it in the donut chart much easier. You can do it in Microsoft Paint in Windows and Preview on Mac.\n\n\nggplot(dt) +\n  geom_arc_bar(aes(x0 = 0, y0 = 0, r0 = 0.8, r = 1, \n                   amount = electoral_votes, \n                   fill = candidate), \n               stat = 'pie',\n               color = \"white\") +\n  geom_label(x = c(0.85, -0.85), y = c(-0.3, 0.3), aes(label = electoral_votes)) +\n  annotation_raster(png::readPNG(here::here(\"Images\", \"biden-pixabay-square.png\")),\n                    xmin = -0.55, xmax = 0.55, ymin = -0.55, ymax = 0.55) +\n  scale_fill_manual(values = c(\"#004BA8\", \"#e63946\")) +\n  theme_void() + \n  coord_fixed() +\n  labs(title = \"2020 US Presidential Elections Electoral Votes\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(color = \"white\", hjust = 0.5),\n        plot.background = element_rect(fill = \"#1d3557\"),\n        panel.background = element_rect(fill = \"#1d3557\"),\n        panel.border = element_blank())\n\n\n\n\n\n\n\n",
    "preview": "posts/2020-12-12-ggorce/distill-preview.png",
    "last_modified": "2020-12-12T14:39:03-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-12-02-election-donuts-pics/",
    "title": "Donut Chart and Geofacets with Images",
    "description": "In this post, I recreate the donut chart overlaid on geo facets. Based on a friend's feedback, I now add pictures of winners of each state.",
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": "www.ashwinmalshe.com"
      }
    ],
    "date": "2020-12-02",
    "categories": [],
    "contents": "\nFor the background and more details on this plot, please check out my earlier blog post: https://dataviz.school/posts/2020-11-20--us-election-donut/\nGetting the data\nAs of the date of this writing (21st November 2020), the results of the US Presidential elections have not tallied. The counting is still going on in a few states. However, it is unlikely that the results will change significantly from this point onward. I decided to get data from this Github repo, which scrapes data from NYT. The data is at county-level: https://github.com/favstats/USElection2020-NYT-Results\nFinal plot\nThis is the plot I created. I have used Biden and Trump pictures from www.pixabay.com. Check it out for a lot of free, attribution-free images. I also tried using Trump and Biden icons but it did not work well. I got Trump and Biden icons from here respectively:\nTrump icon Biden icon\nThese are free to use with attribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode to create the plot\nHere is the code to recreate these plots. Note that I am assuming that you are using the code to prepare data from my previous post.\nThe main challenge in creating this plot is mismatch of the coordinate system. The donuts are created using polar coordinates. It‚Äôs not possible to overlay images on top of those donuts in ggplot2. I decided to use a workaround, which I have successfully used in the past to create dataviz for my wife. I first created two separate plots. First plot is the main donut chart which I created previously. The second plot is just a facet plot of Biden and Trump images. Next I used cowplot package to overlay these two plots on top of each other. It may sound pretty easy but it took me a lot of time to align those two plots perfectly. The key was to use coord_fixed() in the second plot.\nCreate a data set with images\nHere I am creating a data set with a column containing links to either Biden or Trump images. I am also making sure that I retain the winner in each state and DC. The resulting data has 51 rows. Note that in the plot I use image2 variable to overlay images. If you instead want the icons, use image in geom_image() in the code below.\n\n\ndt3 <- dt2 %>%\n  arrange(state, -votes) %>% \n  group_by(state) %>% \n  filter(row_number() == 1) %>% \n  ungroup() %>% \n  mutate(image = case_when(Candidate == \"Biden\" ~ \"https://github.com/ashgreat/dataviz-blog/raw/main/Images/joe-biden.png\",\n                           Candidate == \"Trump\" ~ \"https://github.com/ashgreat/dataviz-blog/raw/main/Images/donald-trump.png\"),\n         image2 = case_when(Candidate == \"Biden\" ~ \"https://github.com/ashgreat/dataviz-blog/raw/main/Images/biden-small-pixabay.png\",\n                           Candidate == \"Trump\" ~ \"https://github.com/ashgreat/dataviz-blog/raw/main/Images/trump-small-pixabay.png\")\n         )\n\n\n\nCreate donut charts\nThis code is basically copied from the previous post.\n\n\ng3 <- dt2 %>% \n  group_by(state) %>% \n  arrange(Candidate) %>% \n  mutate(ymax = cumsum(per_votes),\n         ymin = ifelse(row_number() == 1, 0, lag(ymax)),\n         ypos = (ymin + ymax) / 2) %>% \n  ungroup() %>% \n  ggplot(aes(ymin = ymin, ymax = ymax, xmin = 3, xmax = 4)) +\n  geom_rect(aes(fill = Candidate)) +\n  geom_text(x = 5, \n             aes(y = ypos, \n                 label = formattable::percent(round(per_votes, 2), digits = 0)),\n             size = 2, color = \"white\") +\n  coord_polar(theta = \"y\") +\n  facet_geo(~state) +\n  scale_fill_manual(values = c(\"#0066f2\", \"#e6f1fd\", \"#ff0000\")) +\n  theme_void()+\n  xlim(-1, 5) +\n  labs(caption = \"Ashwin Malshe \\nhttps://dataviz.school\",\n       subtitle = \" \") +\n  theme(legend.text = element_text(family = \"proxima\", size = 10, color = \"white\"),\n        legend.title = element_blank(),\n        legend.direction = \"horizontal\",\n        legend.position = c(0.2, 1),\n        plot.caption = element_text(family = \"proxima\", size = 10, hjust = 0.95,\n                                    margin = margin(0, 0, 5, 0, \"pt\"),\n                                    face = \"bold\", color = \"#69fffb\"),\n        strip.text = element_text(family = \"proxima\", size = 9, color = \"white\",\n                                  margin = margin(0, 0, 5, 0, \"pt\")),\n        strip.background = element_blank(),\n        plot.background = element_rect(fill = \"#2e3440\", color = NA),\n        panel.background = element_rect(fill = \"#2e3440\", color = NA))\n\n\n# Print the plot\n\ng3\n\n\n\nCreate the plot of images\nThis is the plot that will be layered on top of the donut chart. Note geon_image() from ggimage package.\n\n\ng4 <- ggplot(dt3) +\n  facet_geo(~state) +\n  ggimage::geom_image(aes(x = 0.5, y = -1, image = image), size = 0.5) +\n  theme_void()+\n  xlim(0, 1) +\n  coord_fixed() + # This is critical!\n  theme(\n        strip.background = element_blank(),\n        strip.text = element_blank(),\n        panel.background = element_blank(),\n        plot.background = element_blank())\n\n\n\nSuperimposing the two plots\nThis was easy. I just layered g4 on g3 using align_plots() from the fatnastic package cowplot. Finally, I plotted them and saved in an object g5. Note that if you use the same font as the one I used here, cowplot will generate many error messages. Ignore them as it won‚Äôt affect your output.\n\n\naligned_plots <- cowplot::align_plots(g3, g4, align=\"hv\", greedy = FALSE)\n\ng5 <- ggdraw(aligned_plots[[1]]) +\n  draw_plot(aligned_plots[[2]])\n\n\n\nI hope you enjoyed this post!\n\n\n\n",
    "preview": "posts/2020-12-02-election-donuts-pics/distill-preview.png",
    "last_modified": "2020-12-02T15:34:33-06:00",
    "input_file": {},
    "preview_width": 2112,
    "preview_height": 1632
  },
  {
    "path": "posts/2020-11-20--us-election-donut/",
    "title": "Donut Chart and Geofacets",
    "description": "How to combine a donut chart with geofacets in R.",
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": "www.ashwinmalshe.com"
      }
    ],
    "date": "2020-11-19",
    "categories": [],
    "contents": "\nThere are many different ways people have visualized US presidential elections results on the US map. One critical drawback in many of these visualizations is that they show only the results for the winners. I wanted to show the vote percentages for Biden, Trump, and other candidates. These can be easily captured using a pie chart or a donut chart. However, superimposing the charts on the US map is difficult because the sizes of the states vary quite a lot. So I decided to use the fantastic geofacet package, which makes this task easy.\nGetting the data\nAs of the date of this writing (21st November 2020), the results of the US Presidential elections have not tallied. The counting is still going on in a few states. However, it is unlikely that the results will change significantly from this point onward. I decided to get data from this Github repo, which scrapes data from NYT. The data is at county-level: https://github.com/favstats/USElection2020-NYT-Results\nI am reading the data directly into R.\n\n\ndt <- readr::read_csv(\"https://bit.ly/2UO2Zyp\")\n\n\n\n\n\n\nCleaning up the data\nI clean up the data in multiple steps using dplyr:\nGet total votes and absentee votes for all the contentstansts other than Trump and Biden.\nstate names have - in place of a space. For instance, New York is written ‚Äúas new-york‚Äù. Replace all the hyphens with spaces.\nUse title case for all the state names. This screws up District of Columbia by capitalizing ‚ÄúO‚Äù in of. Fix that.\nSummarize the votes at the state level.\nReshape the data using pivot_longer. This will lead to only five columns.\nFinally, calculate the percentage votes.\n\nAbsentee votes for some states were negative so I decided not to use absentee votes in any visualization.\n\n\n# Load the libraries\npacman::p_load(tidyverse, showtext, geofacet)\n\ndt2 <- dt %>% \n  mutate(\n         Others = votes  - (results_trumpd + results_bidenj),\n         Others_ab = absentee_votes - (results_absentee_trumpd + results_absentee_trumpd),\n         state = stringr::str_replace_all(state, \"-\", \" \"),\n         state = stringr::str_to_title(state),\n         state = ifelse(state == \"District Of Columbia\", \"District of Columbia\",  state)\n         ) %>% \n  group_by(state) %>% \n  summarize(Trump_votes = sum(results_trumpd, na.rm = TRUE),\n            Trump_abvotes = sum(results_absentee_trumpd, na.rm = TRUE),\n            Biden_votes = sum(results_bidenj, na.rm = TRUE),\n            Biden_abvotes = sum(results_absentee_bidenj, na.rm = TRUE),\n            Others_votes = sum(Others , na.rm = TRUE),\n            Others_abvotes = sum(Others_ab , na.rm = TRUE),\n            .groups = \"drop\") %>% \n  pivot_longer(cols = c(Trump_votes, Biden_votes, Others_votes,\n                        Trump_abvotes, Biden_abvotes, Others_abvotes),\n               names_to = c(\"Candidate\", \".value\"),\n               names_pattern = \"(.+)_(.+)\") %>% \n  group_by(state) %>% \n    mutate(per_votes = votes / sum(votes)) %>% \n  ungroup()\n\n\n\nCreating the plots\nI am starting off by importing Proxima Nova Condensed font. If you don‚Äôt have this font, use whichever font you like. I recommend using a condensed font. A popular alternative is Robot Condensed.\nI also start showtext.\n\n\nfont_add(\"proxima\", here::here(\"Icons\", \"ProximaNovaCond-Regular.otf\"))\nshowtext_auto()\n\n\n\nNow we are reading to create the plot. Recall that I am overlaying donut charts on the US map but instead of actually using the map, I will instead use geofacet package. This allows us to position facets in the general location of states on the US map.\nI like this package because due to the distortion introduced by the map projections, many states on the US map look smaller than they are. A few states are indeed small. Also, Alaska and Hawaii are so far away from the continental US that it becomes difficult to show them in one map unless we make some adjustments.\nI will show you two different methods to create this graph.\nMethod 1\nIn this method, I will first create a bar graph and then use polar coordinates to convert them into a pie chart. Next, using xlim() function, I will convert the pie chart into a donut chart. Play around with the values inside xlim in the code below to see how the plot changes.\nThis plot will not put the vote percentages as labels on the plot, which will make the plots a bit less interesting. In the next method I will show you how to put the value labels.\n\n\ng1 <- dt2 %>% \n  group_by(state) %>% \n  arrange(Candidate) %>% \n  ungroup() %>% \n  ggplot(aes(x = 1.4, y = per_votes, fill = Candidate)) +\n  geom_col(color = \"white\", width = 0.7) +\n  coord_polar(theta = \"y\", start = 0) +\n  facet_geo(~state) +\n  scale_fill_manual(values = c(\"#0066f2\", \"#e9c41d\", \"#ff0000\")) +\n  theme_void()+\n  xlim(0, 2) +\n  labs(caption = \"Ashwin Malshe \\nhttps://dataviz.school\",\n       subtitle = \" \") +\n  theme(legend.text = element_text(family = \"proxima\", size = 10),\n        legend.title = element_blank(),\n        legend.direction = \"horizontal\",\n        legend.position = c(0.2, 1),\n        plot.caption = element_text(family = \"proxima\", size = 10, hjust = 0.95,\n                                    margin = margin(0, 0, 5, 0, \"pt\"),\n                                    face = \"bold\", color = \"#1500f4\"),\n        strip.text = element_text(family = \"proxima\", size = 9,\n                                  margin = margin(0, 0, 5, 0, \"pt\"))) \n\n# Print the plot\ng1\n\n\n\n\n\n\n\nIf you like it, save the plot using ggsave() function.\nMethod 2\nIn the second method, I will use geom_rect to add rectangles first and then use polar coordinates to create a pie chart. Once again he limits specified inside xlim() will convert it into a donut chart.\n\n\ng2 <- dt2 %>% \n  group_by(state) %>% \n  arrange(Candidate) %>% \n  mutate(ymax = cumsum(per_votes),\n         ymin = ifelse(row_number() == 1, 0, lag(ymax)),\n         ypos = (ymin + ymax) / 2,\n         ypos = ifelse(state == \"District of Columbia\" & Candidate == \"Trump\",\n                       0.05, ypos)) %>% \n  ungroup() %>% \n  ggplot(aes(ymin = ymin, ymax = ymax, xmin = 3, xmax = 4, fill = Candidate)) +\n  geom_rect() +\n  geom_text(x = 1.8, \n             aes(y = ypos, label = formattable::percent(round(per_votes, 2), digits = 0)),\n             size = 2) +\n  coord_polar(theta = \"y\") +\n  facet_geo(~state) +\n  scale_fill_manual(values = c(\"#0066f2\", \"#e6f1fd\", \"#ff0000\")) +\n  theme_void()+\n  xlim(-1, 4) +\n  labs(caption = \"Ashwin Malshe \\nhttps://dataviz.school\",\n       subtitle = \" \") +\n  theme(legend.text = element_text(family = \"proxima\", size = 10),\n        legend.title = element_blank(),\n        legend.direction = \"horizontal\",\n        legend.position = c(0.2, 1),\n        plot.caption = element_text(family = \"proxima\", size = 10, hjust = 0.95,\n                                    margin = margin(0, 0, 5, 0, \"pt\"),\n                                    face = \"bold\", color = \"#1500f4\"),\n        strip.text = element_text(family = \"proxima\", size = 9,\n                                  margin = margin(0, 0, 5, 0, \"pt\")))\n\n\n# Print the plot\n\ng2\n\n\n\n\n\n\n\nAnother version of the same plot with a different background.\n\n\ng3 <- dt2 %>% \n  group_by(state) %>% \n  arrange(Candidate) %>% \n  mutate(ymax = cumsum(per_votes),\n         ymin = ifelse(row_number() == 1, 0, lag(ymax)),\n         ypos = (ymin + ymax) / 2,\n                  ypos = ifelse(state == \"District of Columbia\" & Candidate == \"Trump\",\n                       0.05, ypos)) %>% \n  ungroup() %>% \n  ggplot(aes(ymin = ymin, ymax = ymax, xmin = 3, xmax = 4, fill = Candidate)) +\n  geom_rect() +\n  geom_text(x = 1.8, \n             aes(y = ypos, \n                 label = formattable::percent(round(per_votes, 2), digits = 0)),\n             size = 2, color = \"white\") +\n  coord_polar(theta = \"y\") +\n  facet_geo(~state) +\n  scale_fill_manual(values = c(\"#0066f2\", \"#e6f1fd\", \"#ff0000\")) +\n  theme_void()+\n  xlim(-1, 4) +\n  labs(caption = \"Ashwin Malshe \\nhttps://dataviz.school\",\n       subtitle = \" \") +\n  theme(legend.text = element_text(family = \"proxima\", size = 10, color = \"white\"),\n        legend.title = element_blank(),\n        legend.direction = \"horizontal\",\n        legend.position = c(0.2, 1),\n        plot.caption = element_text(family = \"proxima\", size = 10, hjust = 0.95,\n                                    margin = margin(0, 0, 5, 0, \"pt\"),\n                                    face = \"bold\", color = \"#a3be8c\"),\n        strip.text = element_text(family = \"proxima\", size = 9, color = \"white\",\n                                  margin = margin(0, 0, 5, 0, \"pt\")),\n        plot.background = element_rect(fill = \"#2e3440\", color = NA),\n        panel.background = element_rect(fill = \"#2e3440\", color = NA) )\n\n\n# Print the plot\n\ng3\n\n\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2020-11-20--us-election-donut/distill-preview.png",
    "last_modified": "2020-11-21T22:03:36-06:00",
    "input_file": {},
    "preview_width": 2112,
    "preview_height": 1632
  },
  {
    "path": "posts/2020-10-31-plotting-election-win-probabilities/",
    "title": "Mapping Election Win Probabilities",
    "description": "I show how to create a election map using ggplot2",
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": "www.ashwinmalshe.com"
      }
    ],
    "date": "2020-10-31",
    "categories": [],
    "contents": "\nIn this post, I will show you how to make a map using ggplot2. For this, I am going to use an excellent post by Andrew Gelman.\nFivethirtyeight daily updates the probabilities of Trump and Biden wins based on multiple polls and their own model.1 They also make available their simulation outcomes as a json file. Next, I merged the cleaned simulation data with US map data. I will create a separate post to show you how to read, clean, and merge these data sets.\nRead the data sets and load packages\nI have saved all the required data sets as a .rda file on Github. You can directly read them into your R code without saving it first.\n\n\nload(url(\"https://github.com/ashgreat/DA6233/blob/master/sim-elect.rda?raw=true\"))\n\n\n\nI will use ggrepel package to make sure that the state labels don‚Äôt overlap. If you don‚Äôt have this package installed, run install.packages(\"ggrepel\") in the RStudio console.\n\n\nlibrary(tidyverse)\nlibrary(ggrepel)\n\n\n\nMaking the map\nNow we are ready to make the map. We will use trump_wins data set. This data set has results of 40,000 simulations. Each simulation shows whether Trump will win or lose a state. There are results for 50 states and Washington D.C. There are five more results for, what I think is, regional data. We will not use those results.\nTrump probability of winning each state is simply the column means of this data set. In the following code the first four lines calculate the probabilities and restructure the data so that it is easier to merge with the data on maps.\n\n\ndt <- trump_wins %>% \n  summarize(across(everything(), mean)) %>%\n  mutate(across(everything(), ~ round(.x * 100, 2))) %>% \n  pivot_longer(cols = everything(), names_to = \"state_abb\", values_to = \"prob\") %>% \n  inner_join(select(state_names, -lat, -long), by = \"state_abb\") %>% \n  inner_join(fifty_states, by = c(\"state_low\" = \"id\"))\n\n\n\nFinally, we create a map using dt. The labels of the states are split into two groups. Many northeastern states show up small on the map so their names are no readable easily. Therefore, we need to show them a little bit away from the map. For this we will use geom_text_repel() function from ggrepel package.\n\n\ndt %>%\n  ggplot(aes(x = long, y = lat)) +\n  geom_polygon(aes(group = group, fill = prob), color = \"#d8dee9\", size = 0.05) +\n  geom_text(data = filter(state_label, \n                          !state_abb %in% c(\"MA\", \"RI\", \"CT\", \"NJ\", \"DE\", \"DC\", \"MD\")), \n            aes(label = state_abb), \n            size = 3, hjust = 0.5, family = \"Roboto Condensed\") +\n  geom_text_repel(data = filter(state_label, \n                                state_abb %in% c(\"MA\", \"RI\", \"CT\", \"NJ\", \"DE\", \"DC\", \"MD\")),\n                  aes(label = state_abb), \n                  nudge_x = 5, \n                  segment.size  = 0.2, \n                  segment.color = \"grey50\",\n                  direction     = \"y\", \n                  size = 3, \n                  hjust = 0.5, \n                  family = \"Roboto Condensed\") +\n  coord_map(projection = \"albers\", lat0 = 39, lat1 = 45) +\n  scale_fill_gradient2(low = \"#0063c4\", \n                       mid = \"#d8dee9\", \n                       high = \"#ef2e69\",\n                       midpoint = 50,\n                       labels = c(\"0%\", \"25%\", \"50%\", \"75%\", \"100%\"),\n                       guide = guide_colorbar(barwidth = 10, \n                                              barheight = 0.4,\n                                              title.position = \"top\")) + \n  labs(fill = \"Probability of Trump Winning a State\") +\n  ggthemes::theme_map() +\n  theme(legend.position = c(0.3, -0.2),\n        legend.direction= \"horizontal\",\n        legend.title = element_text(family = \"Roboto Condensed\")) \n\n\n\n\n\nYou can check out the maps and other information on their website: https://projects.fivethirtyeight.com/trump-biden-election-map/‚Ü©Ô∏é\n",
    "preview": "posts/2020-10-31-plotting-election-win-probabilities/plotting-election-win-probabilities_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2020-11-01T14:40:24-06:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 1152
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to R Data Visualization",
    "description": "Welcome to my new blog, R Data Visualization. I hope youe enjoy the code\nand visualizations!",
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": "www.ashwinmalshe.com"
      }
    ],
    "date": "2020-10-31",
    "categories": [],
    "contents": "\nMy main website is ashwinmalshe.com. However, due to some configurations issues, I am unable to update it. It has made it difficult for me to post new blog posts. Therefore, while I am porting my website to another project, I decided to start this blog. Here I am gradually making available all the old posts from my website. In addition, I will add new material.\n\n\n\n",
    "preview": {},
    "last_modified": "2020-10-31T23:07:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-31-plotting-covid-19-pandemic/",
    "title": "Plotting Covid-19 Pandemic",
    "description": "Interactive plot of Covid-19 cases",
    "author": [
      {
        "name": "Ashwin Malshe",
        "url": "www.ashwinmalshe.com"
      }
    ],
    "date": "2020-03-08",
    "categories": [],
    "contents": "\nIn this post, we will visualize spread of worldwide COVID-19 cases through time. I obtained the data from Rami Krispin‚Äôs website: https://ramikrispin.github.io/coronavirus/ using coronovirus package. I also decided to do some experimentation using John Coene‚Äôs fantastic echarts4r package, which allows us to access echarts API.\nLoad the libraries and get the data in the R session.\n\n\nlibrary(dplyr)\nlibrary(echarts4r)\nlibrary(coronavirus)\n\n# Get the data\ndata(\"coronavirus\")\n\n\n\nData Preparation\nPrint out the first 6 observations.\n\n\nhead(coronavirus)\n\n\n        date province     country      lat     long      type cases\n1 2020-01-22          Afghanistan 33.93911 67.70995 confirmed     0\n2 2020-01-23          Afghanistan 33.93911 67.70995 confirmed     0\n3 2020-01-24          Afghanistan 33.93911 67.70995 confirmed     0\n4 2020-01-25          Afghanistan 33.93911 67.70995 confirmed     0\n5 2020-01-26          Afghanistan 33.93911 67.70995 confirmed     0\n6 2020-01-27          Afghanistan 33.93911 67.70995 confirmed     0\n\nWe are interested in date and type. Let‚Äôs take a look at the distinct values for type.\n\n\ncoronavirus %>% count(type)\n\n\n       type     n\n1 confirmed 75576\n2     death 75576\n3 recovered 71910\n\nThere are only 3 values: confirmed, death, and recovered. Next we will create sum of cases for each of the values and store them in separate data sets.\n\n\ndt1 <- coronavirus %>% \n  filter(type == \"confirmed\") %>% \n  group_by(date) %>% \n  summarize(Confirmed = sum(cases, na.rm = TRUE), .groups = \"drop\")\n\ndt2 <- coronavirus %>% \n  filter(type == \"death\") %>% \n  group_by(date) %>% \n  summarize(Death = sum(cases, na.rm = TRUE), .groups = \"drop\")\n\n\ndt3 <- coronavirus %>% \n  filter(type == \"recovered\") %>% \n  group_by(date) %>% \n  summarize(Recovered = sum(cases, na.rm = TRUE), .groups = \"drop\")\n\n\n\nFinally, we will merge the 3 datasets so that we will have the counts of each type in separate columns.\n\n\ndt <- dt1 %>% \n  inner_join(dt2, by = \"date\") %>% \n  inner_join(dt3, by = \"date\")\n\n\n\nPlot\nFinally, time to make the plot! Note how we can build this plot in separate elements.\n\n\ndt %>% \n  e_charts(x = date) %>% \n  e_line(serie = Confirmed) %>% \n  e_line(serie = Death) %>% \n  e_line(serie = Recovered) %>% \n  e_tooltip(trigger = \"axis\") %>% \n  e_datazoom(type = \"slider\") %>% \n  e_title(\"Worldwide COVID-19 cases\") %>% \n  e_theme(\"bee-insipired\") \n\n\n\n\n\nThis plot is interactive so you can hover over the plot to get the exact readings. You can also toggle time series on or off by clicking on the legends on top.\n\n\n\n",
    "preview": {},
    "last_modified": "2020-10-31T23:04:43-05:00",
    "input_file": {}
  }
]
